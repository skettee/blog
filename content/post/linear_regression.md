
+++
title = "ì„ í˜• íšŒê·€ (Linear Regression)"
date = "2019-8-15"
author = "skettee"
categories = ["Deep Learning", "Linear Regression"]
tags = ["ë”¥ëŸ¬ë‹", "ì„ í˜•íšŒê·€", "ì†ì‹¤ í•¨ìˆ˜", "í‰ê·  ì œê³± ì˜¤ì°¨", "ê²½ì‚¬í•˜ê°•ë²•"]
+++



ë”¥ëŸ¬ë‹ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°€ê¸° ìœ„í•´ ì•Œì•„ì•¼ í•˜ëŠ” ì²«ë²ˆì§¸ ëª¨ë¸ì¸ ì„ í˜• íšŒê·€(Linear Regression)ì— ëŒ€í•´ ì•Œì•„ë³´ê³  kerasë¥¼ ì´ìš©í•´ì„œ ëª¨ë¸ë§ì„ í•´ë³´ì!
<!--more-->

ì‹¤ì œë¡œ ëŒë ¤ ë³´ê³  ì‹¶ìœ¼ë©´ êµ¬ê¸€ ì½”ë©ìœ¼ë¡œ ~  

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skettee/notebooks/blob/master/linear_regression.ipynb)


## ë¬¸ì œ (Problem)

ğŸ’° ê³ ê°

> ìš°ë¦¬ í•™êµì—ëŠ” 'ëª¸ì§±ë°˜'ì´ ìˆì–´ìš”.   
> ì—¬ê¸°ì— ë“¤ì–´ê°€ê¸° ìœ„í•´ì„œëŠ”   
> í‚¤ì™€ ëª¸ë¬´ê²Œì˜      
> íŠ¹ë³„í•œ ì¡°ê±´ì´ ìˆëŠ”ê±° ê°™ì•„ìš”.  
>
> 'ëª¸ì§±ë°˜'ì— ë“¤ì–´ê°„ ì¹œêµ¬ë“¤ì˜   
> í‚¤ì™€ ëª¸ë¬´ê²Œ ë°ì´í„°ë¥¼ ê°€ì§€ê³    
> í‚¤ê°€ ì£¼ì–´ì§€ë©´   
> 'ëª¸ì§±ë°˜'ì— ë“¤ì–´ê°€ê¸° ìœ„í•œ ëª¸ë¬´ê²Œë¥¼  
> ìë™ìœ¼ë¡œ ì•Œê³  ì‹¶ì–´ìš”...  
>
> ë§Œë“¤ì–´ ì¤„ ìˆ˜ ìˆì§€ìš”? 
>
> ë°ì´í„°ëŠ” ì•„ë˜ì— ìˆì–´ìš”


```python
height_data=  [150.0, 150.8, 151.6, 152.4, 153.2, 154.0, 154.8, 155.7, 156.5, 157.3, 158.1, 158.9, 159.7, 160.6, 161.4, 162.2, 163.0, 163.8, 164.6, 165.5, 166.3, 167.1, 167.9, 168.7, 169.5, 170.4, 171.2, 172.0, 172.8, 173.6, 174.4, 175.3, 176.1, 176.9, 177.7, 178.5, 179.3, 180.2, 181.0, 181.8, 182.6, 183.4, 184.2, 185.1, 185.9, 186.7, 187.5, 188.3, 189.1, 190.0]
weight_data=  [45.198, 46.212, 45.850, 47.389, 46.920, 49.275, 49.790, 50.354, 51.694, 50.789, 52.301, 52.015, 54.277, 54.879, 56.066, 55.140, 55.808, 56.516, 57.401, 59.633, 59.815, 59.843, 61.265, 61.569, 62.276, 63.741, 64.764, 64.674, 65.149, 66.910, 66.000, 67.076, 69.120, 69.940, 69.820, 69.971, 70.625, 71.856, 73.483, 73.709, 74.028, 75.685, 76.123, 76.346, 76.917, 78.641, 78.157, 79.479, 80.725, 80.582]
```

## ë°ì´í„° ë¶„ì„ (Data Analysis)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë°ì´í„°ê°€ ì–´ë–¤ ëª¨ì–‘ì¸ì§€    
> í™•ì¸í•´ ë³´ì•„ì•¼ ê² êµ°


```python
%matplotlib inline

import matplotlib.pyplot as plt

plt.scatter(height_data, weight_data)
plt.xlabel('height (cm)')
plt.ylabel('weight (kg)')
plt.show()
```


![png](output_7_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> í‚¤ê°€ ì»¤ì§ˆ ìˆ˜ë¡ ëª¸ë¬´ê²Œê°€ ì„ í˜•(Linear)ì ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ë„¤~   
> ì§ì„ ìœ¼ë¡œ ëª¨ë¸ë§ì„ í•  ìˆ˜ ìˆê² ë‹¤.
> 
> ëª¨ë¸ë§ ì‘ì—…ì´ í¸ì•ˆí•˜ë„ë¡ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ì!

## ë°ì´í„° ë³€í™˜ (Data Transformation)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> Keras, tensorflowë“±ì˜ ë¨¸ì‹ ëŸ°ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ”  
> ê°ê°ì´ ìš”êµ¬í•˜ëŠ” ë°ì´í„°ì˜ í˜•ì‹ ë° ëª¨ì–‘ì— ë§ì¶”ì–´ì„œ ë³€í™˜ì„ í•´ ì£¼ì–´ì•¼ í•œë‹¤.  
> ëª¨ë¸ë§ì„ í•˜ë©´ì„œ ê³¨ì¹˜ ì•„í”ˆ ê²ƒ ì¤‘ì— í•˜ë‚˜ëŠ”  
> ì…ë ¥ê³¼ ì¶œë ¥ì˜ ëª¨ì–‘(Shape)ë¥¼ ë§ì¶”ì–´ ì£¼ëŠ” ê²ƒì´ë‹¤.  
> ë³µì¡í•˜ë‹¤ ... ì˜ëª»í•˜ë©´ ë‹¤í¬-ë””ë©˜ì…˜ì— ë¹ ì§„ë‹¤...   
> ë’¤ì—ì„œ ë” ìì„¸í•˜ê²Œ ì„¤ëª…í•  ê¸°íšŒê°€ ìˆì„ ê²ƒì´ë‹¤.  
>
> ì¼ë‹¨ ì—¬ê¸°ì„œëŠ”  
> í‚¤ì™€ ëª¸ë¬´ê²Œ ë°ì´í„°ë¥¼ ê°ê° ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•˜ì  
> í–‰ì˜ í¬ê¸°ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜    
> ì—´ì˜ í¬ê¸°ëŠ” ì¸¡ì •í•œ í•­ëª©ì˜ ê°œìˆ˜    
>  
> í‚¤ ê°’ì„ ì…ë ¥í•˜ë©´ 'ëª¸ì§±ë°˜'ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ëª¸ë¬´ê²Œë¥¼ ì˜ˆì¸¡í•´ì•¼ í•˜ë‹ˆê¹Œ...  
> í‚¤ ë°ì´í„°ë¥¼ ì…ë ¥ xë¼ê³  í•˜ê³  ëª¸ë¬´ê²Œ ë°ì´í„°ë¥¼ ì¶œë ¥ yë¼ê³  í•˜ì  
>
> í‚¤ ë°ì´í„°ëŠ” 50ê°œì˜ 'í‚¤'ë¥¼ ì¸¡ì •í•œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ 50X1 ë§¤íŠ¸ë¦­ìŠ¤   
> ëª¸ë¬´ê²Œ ë°ì´í„°ëŠ” 50ê°œì˜ 'ëª¸ë¬´ê²Œ'ë¥¼ ì¸¡ì •í•œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ 50X1 ë§¤íŠ¸ë¦­ìŠ¤ì´ë‹¤.  


```python
import numpy as np

x = np.array(height_data).reshape(len(height_data), 1)
y = np.array(weight_data).reshape(len(weight_data), 1)

print('x = ', x[:10])
print('x.shape= ', x.shape)
print('\ny = ', y[:10])
print('y.shape= ', y.shape)
```

    x =  [[150. ]
     [150.8]
     [151.6]
     [152.4]
     [153.2]
     [154. ]
     [154.8]
     [155.7]
     [156.5]
     [157.3]]
    x.shape=  (50, 1)
    
    y =  [[45.198]
     [46.212]
     [45.85 ]
     [47.389]
     [46.92 ]
     [49.275]
     [49.79 ]
     [50.354]
     [51.694]
     [50.789]]
    y.shape=  (50, 1)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì¢‹ì•˜ì–´!   
> 
> ê·¸ëŸ¼ ë³€í™˜ëœ ë°ì´í„°ë¡œ ì„ í˜•íšŒê·€(Linear Regression) ëª¨ë¸ë§ì„ ì‹œì‘í•´ ë³´ì§€

## ì„ í˜• ëª¨ë¸ë§ (Linear Modeling)

âš™ï¸ ì—”ì§€ë‹ˆì–´) 

> ëª¨ë¸ì„ \\(y=wx+b\\)ë¡œ ë†“ê³    
> ì£¼ì–´ì§„ ë°ì´í„°ì— ê°€ì¥ ë¹„ìŠ·í•œ ì§ì„ ì„ í‘œì‹œí•˜ëŠ”   
> \\(w\\)ì™€ \\(b\\)ë¥¼ êµ¬í•˜ë©´ ë!   
> 
> \\(w\\)ëŠ” ì§ì„ ì˜ ê¸°ìš¸ê¸°ê³  \\(b\\)ëŠ” \\(y\\)ì ˆí¸ì´ë¼ê³  ë°°ì› ì§€ë§Œ,   
> ì´ì œ ë¶€í„°ëŠ” \\(w\\)ëŠ” **weight**ê³    
> \\(b\\)ëŠ” **bias**ë¡œ ë¶€ë¥´ë„ë¡ í•˜ê² ë‹¤...   
>
> ì™œ? ë©‹ìˆìœ¼ë‹ˆê¹Œ!    
>
> ê·¸ëŸ°ë°... \\(w\\)ì™€ \\(b\\)ë¥¼ ì–´ë–»ê²Œ êµ¬í•˜ë©´ ë˜ì§€?


### ì†ì‹¤ í•¨ìˆ˜ (Loss function)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë‚¨ì—¬ ì»¤í”Œ 10ëª…ì´ ëª¨ì—¬ì„œ ê²Œì„ì„ ì‹œì‘í•œë‹¤.  
> ë‚¨ìëŠ” ì—¬ìì¹œêµ¬ì˜ ëª¸ë¬´ê²Œë¥¼ ë§í•˜ê³  ê°€ì¥ ë¹„ìŠ·í•œ ëª¸ë¬´ê²Œë¥¼ ë§ì¶”ëŠ” ì»¤í”Œì´ 1ë“±ì´ ëœë‹¤.  
> 1ë“±ì„ ì •í•˜ëŠ” ë°©ë²•ì€ ë‚¨ìê°€ ë§í•œ ëª¸ë¬´ê²Œì™€ ì—¬ìì¹œêµ¬ê°€ ë§í•œ ì‹¤ì œ ëª¸ë¬´ê²Œì˜ ì°¨ì´ë¥¼ ì ìˆ˜ë¡œ ê³„ì‚°í•˜ê³ ,  
> ê°€ì¥ ì ìˆ˜ê°€ ë‚®ì€(ê°€ì¥ ë¹„ìŠ·í•˜ê²Œ ë§ì¶˜) ì»¤í”Œì—ê²Œ 1ë“±ì„ ì£¼ë©´ ëœë‹¤.  
>
> ì‹¤ì œë¡œ ì ìˆ˜ë¥¼ ê³„ì‚°í•  ë•Œì—ëŠ” ë‹¨ìˆœí•œ ì°¨ì´ê°’ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³   
> ì°¨ì´ê°’ì„ ì œê³±í•˜ê³  ë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¤€ ê°’ì„ ì‚¬ìš©í•œë‹¤.    
>
> ê·¸ë¦¬ê³  ì»¤í”Œ 10ëª… ì „ì²´ì— ëŒ€í•œ ì ìˆ˜ëŠ” ê° ì»¤í”Œë“¤ì˜ ì ìˆ˜ì˜ í‰ê· ì„ ë‚´ë©´ ëœë‹¤.

í•˜ë‚˜ì˜ ë°ì´í„° ì„¸íŠ¸(\\(x^{(i)}, y^{(i)}\\))ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì—ì„œ ì–»ì€ ê°’ê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´(Loss)ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í•´ë³´ì.  
ì—¬ê¸°ì„œ \\(x^{(i)}\\)ëŠ” ië²ˆì§¸ \\(x\\)ê°’ì´ê³  \\(y^{(i)}\\)ì€ ië²ˆì§¸ \\(y\\)ê°’ì´ë‹¤.  

ì¼ë‹¨ \\(w\\)ì™€ \\(b\\)ëŠ” ì„ì˜ì˜ ê°’ìœ¼ë¡œ ë†“ì. ê·¸ë¦¬ê³  ëª¨ë¸ì— \\(x^{(i)}\\)ì„ ë„£ê³  ê³„ì‚°í•œ ê²°ê³¼ ê°’ \\({\\hat y}^{(i)}\\)ê³¼ ì‹¤ì œ ê°’ \\(y^{(i)}\\)ì˜ ì°¨ì´ë¥¼ êµ¬í•œë‹¤. ì„ í˜• ëª¨ë¸ë§ì—ì„œëŠ” ì œê³± ì˜¤ì°¨(squred error)ë¥¼ ì‚¬ìš©í•œë‹¤.

\\(L({\\hat y}^{(i)}, y^{(i)})={1\\over2}({\\hat y}^{(i)}-y^{(i)})^2\\)

ëª¨ë“  ë°ì´í„°(mê°œì˜ ë°ì´í„° ì„¸íŠ¸)ë¡œ ë¶€í„° ì–»ì€ ê²ƒì„ í‰ê· (mean squred error)í•œê²ƒì´ ì†ì‹¤ í•¨ìˆ˜(Loss function)ì´ë‹¤. ì†ì‹¤ í•¨ìˆ˜ëŠ” \\(w\\)ì™€ \\(b\\)ì˜ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

\\({\\large J}(w, b) = {1\\over m}\\sum\_{i=1}^m L({\\hat y}^{(i)}, y^{(i)}) \\\\ 
\\\\ 
\\hspace{2.9em}= {1\\over {2m}}\\sum\_{i=1}^m ({\\hat y}^{(i)}-y^{(i)})^2\\)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì†ì‹¤ í•¨ìˆ˜   
> 
> \\({\\large J}(w, b) = {1\\over {2m}}\\sum\_{i=1}^m ({\\hat y}^{(i)}-y^{(i)})^2\\)  
>
> ê°€ ìµœì†Œê°€ ë˜ëŠ” \\(w\\)ì™€ \\(b\\)ë¥¼ ì°¾ìœ¼ë©´ ë˜ê² êµ°!

**ì†ì‹¤ í•¨ìˆ˜ (Loss function) ì‹œê°í™”**

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ìš°ì„  ì†ì‹¤ í•¨ìˆ˜(Loss function)ê°€ ì–´ë–»ê²Œ ìƒê²¨ ë¨¹ì—ˆëŠ”ì§€ ì‚´í´ ë³´ì.  
> xì¶•ì„ \\(w\\)ë¡œ ë†“ê³ , yì¶•ì„ \\(b\\)ë¡œ ë†“ê³ , zì¶•ì„ ì†ì‹¤ í•¨ìˆ˜ \\({\\large J}(w, b)\\)ë¡œ 
> ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ ë³´ë©´  
> ì–´ë–»ê²Œ ìµœì†Œê°’ì„ ì°¾ì„ì§€ ê°ì´ ì˜¬ ê²ƒ ê°™ë‹¤.   
>
> ì¼ë‹¨ \\(w\\)ëŠ”     
> \\(-5\\leqq w \\leqq 5\\) ì •ë„ë¡œ ì¡ê³ ,  
> \\(b\\)ëŠ” \\(-5\\leqq b \\leqq 5\\)ìœ¼ë¡œ ì¡ì•„ë³´ì.  


```python
import numpy as np
from sklearn.metrics import mean_squared_error
from mpl_toolkits import mplot3d

# W,bì˜ ë²”ìœ„ë¥¼ ê²°ì •í•œë‹¤.
w = np.arange(-5.0, 5.0, 0.1)
b = np.arange(-5.0, 5.0, 0.1)
j_array = []

W, B = np.meshgrid(w, b)

# w, bë¥¼ í•˜ë‚˜ì”© ëŒ€ì‘í•œë‹¤.
for we, be in zip(np.ravel(W), np.ravel(B)):
    y_hat = np.add(np.multiply(we, x), be)
    # Loss function
    mse = mean_squared_error(y_hat, y) / 2.0
    j_array.append(mse)

# ì†ì‹¤(Loss)ì„ êµ¬í•œë‹¤.
J = np.array(j_array).reshape(W.shape)

# ì„œí”¼ìŠ¤ ê·¸ë˜í”„ë¥¼ ê·¸ë¦°ë‹¤.
fig = plt.figure()
ax = plt.axes(projection="3d")

ax.plot_surface(W, B, J, color='b', alpha=0.5)
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_zlabel('J')
ax.set_zticks([])
plt.show()
```


![png](output_18_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> U ëª¨ì–‘ìœ¼ë¡œ êµ¬ë¶€ëŸ¬ì§„ ëª¨ì–‘ì´ë‹¤!  
> \\(w\\)ê°€ 1ê·¼ì²˜ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Œê°’ì„ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.  
>
> \\(b\\)ëŠ” ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Œê°€ ë˜ëŠ” ê°’ì´ ì˜ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤.   
> í™•ëŒ€ë¥¼ í•˜ë©´ ë³´ì´ê² ì§€ë§Œ ê·€ì°®ë‹¤.  
>
> ì´ì œ ë¨ë¤í•˜ê²Œ \\(w\\)ì™€ \\(b\\)ê°’ì´ ì£¼ì–´ì§€ë”ë¼ë„   
> ìë™ìœ¼ë¡œ ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ëŠ” ë°©ë²•ì„ ì°¾ìœ¼ë©´ ëœë‹¤.  
>
> ì–´ë–»ê²Œ ìë™ìœ¼ë¡œ ì°¾ì§€?

### ê²½ì‚¬ í•˜ê°•ë²• (Gradient Descent)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë“±ì‚°ì—ì„œ ì“°ëŠ” ë§ì²˜ëŸ¼ ë“¤ë¦¬ëŠ” ì´ ë°©ë²•ì€  
> ë‚´ê°€ í˜„ì¬ ìˆëŠ” ì‚°ì˜ ìœ„ì¹˜ì—ì„œ   
> ê²½ì‚¬(ê¸°ìš¸ê¸°)ë¥¼ í™•ì¸í•˜ê³    
> ê²½ì‚¬ê°€ ì•„ë˜ì¸ ë°©í–¥ìœ¼ë¡œ í•œê±¸ìŒ ë‚´ë ¤ê°€ê³ ,   
> ê²½ì‚¬ë¥¼ í™•ì¸í•˜ê³ ,   
> ë˜ í•œê±¸ìŒ ë‚´ë ¤ê°€ê³ ë¥¼   
> ê³„ì† ë°˜ë³µí•˜ë©´   
> ê²°êµ­ ê³¨ì§œê¸° ê°€ì¥ ì•„ë˜ë¡œ ë„ì°©í•  ìˆ˜ ìˆë‹¤ëŠ” ë°©ë²•ì´ë‹¤.    
>
> ì—¬ê¸°ì„œ ê²½ì‚¬ë¥¼ í™•ì¸ í•˜ëŠ” ë°©ë²•ì´   
> **ë¯¸ë¶„**ì´ë‹¤.   
> 
> ì˜ ëª¨ë¥´ê² ë‹¤ê³ ?   
> ì•„ë˜ì— ê°„ë‹¨í•œ ì˜ˆì œë¥¼ ë³´ì

**ì´ì°¨ í•¨ìˆ˜ì—ì„œ ë¯¸ë¶„ìœ¼ë¡œ ìµœì†Œê°’ ì°¾ê¸°**

\\(y=x^2-2x+4\\)ì—ì„œ \\(y\\)ê°€ ìµœì†Œê°€ ë˜ëŠ” \\(x\\)ê°’ì„ ë¯¸ë¶„ìœ¼ë¡œ êµ¬í•´ ë³´ì.  
* ì°¸ê³ ë¡œ \\(y=(x-1)^2 +3\\)ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ \\(x=1\\)ì¼ë•Œ \\(y\\)ëŠ” ìµœì†Œê°’ 3ì„ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.  

![ì´ì°¨ í•¨ìˆ˜ì—ì„œ ë¯¸ë¶„ìœ¼ë¡œ ìµœì†Œê°’ ì°¾ê¸°](https://skettee.github.io/post/linear_regression/gd_graph.png)

1. \\(x\\)ê°’ì„ ì•„ë¬´ê±°ë‚˜ í•˜ë‚˜ ì„ íƒí•œë‹¤. ì—¬ê¸°ì„œëŠ” \\(x=3\\)ì„ ì„ íƒí•˜ì
2. \\(x=3\\)ì¼ë•Œ ìˆœê°„ ë³€í™”ìœ¨ì„ ê³„ì‚°í•œë‹¤. ìˆœê°„ ë³€í™”ìœ¨ì€ ë¯¸ë¶„ì´ë‹¤. \\({dy\\over dx}=2x-2\\) ì´ë¯€ë¡œ ìˆœê°„ ë³€í™”ìœ¨ì€ 4ì´ë‹¤.
3. ìˆœê°„ ë³€í™”ìœ¨ì´ 0ë³´ë‹¤ í¬ë©´ \\(x\\)ë¥¼ -ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤. ì´ë–„ ì–¼ë§ˆë‚˜ ì´ë™í•˜ëŠëƒê°€ ì¤‘ìš”í•œë° ìˆœê°„ ë³€í™”ìœ¨ 4ë§Œí¼ ì´ë™í•˜ë©´ ìµœì†Œê°’ì„ ì§€ë‚˜ì³ ë²„ë¦°ë‹¤. ë”°ë¼ì„œ ì¡°ê¸ˆì”© ì›€ì§ì´ë„ë¡ ìˆœê°„ ë³€í™”ìœ¨ì— 0.01ë°° ë§Œí¼ ì›€ì§ì´ê²Œ í•œë‹¤. (\\(x\\)ë¥¼ 0.04 ë§Œí¼ -ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤. ì¦‰ \\(x=2.96\\))
4. ìˆœê°„ ë³€í™”ìœ¨ì´ 0ë³´ë‹¤ ì‘ìœ¼ë©´ xë¥¼ +ë°©í–¥ìœ¼ë¡œ ì´ë™í•œë‹¤. ìœ„ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë¯¸ë¶„ê°’ì— 0.01ë°° ë§Œí¼ ì›€ì§ì´ê²Œ í•œë‹¤. ì—¬ê¸°ì„œ 0.01ì„ learning rateë¼ê³  í•˜ê³  \\(\\alpha\\)ë¡œ í‘œì‹œí•œë‹¤.
5. 2ë²ˆì—ì„œ 4ë²ˆê¹Œì§€ì˜ ê³¼ì •ì„ **epoch** ë¼ê³  í•œë‹¤.
6. ì—…ë°ì´íŠ¸ ëœ xê°’ì„ ê°€ì§€ê³  2, 3, 4ë²ˆì„ ë°˜ë³µí•œë‹¤. 1000ë²ˆ ë°˜ë³µí•œë‹¤. (1000 epoch)


```python
def derivative(x):
    dydx = 2*x-2
    return dydx
    
epoch = 1000 # ë°˜ë³µ íšŒìˆ˜
learning_rate = 0.01 # alpha

xx = 3 # ì´ˆê¸° xê°’

for i in range(epoch):
    xx = xx - learning_rate * derivative(xx)
    
print('x for minimum y is: ', xx)
```

    x for minimum y is:  1.0000000033659349


âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì¢‹ì•˜ì–´!   
>
> \\(w\\)ì™€ \\(b\\)ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ìˆœê°„ ë³€í™”ìœ¨(ë¯¸ë¶„ê°’)ì„ êµ¬í•´ì„œ   
> ë°˜ë³µí•´ì„œ ì—…ë°ì´íŠ¸ë¥¼ í•´ ì£¼ë©´   
> ìë™ìœ¼ë¡œ ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Œ ê°’ì— ë„ë‹¬ í•  ìˆ˜ ìˆê² ì–´!   
> 
> ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•´ë³´ì
>
> REPEAT(epoch) {    
> \\(w:=w-\\alpha {\\partial {J(w,b)}\\over \\partial w}\\)    
>
> \\(b:=b-\\alpha {\\partial{J(w,b)}\\over \\partial b}\\) , \\(\\alpha=0.01\\) learining rate    
> }
>
> ì•„í•˜! **ì†ì‹¤í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ì•„ ê°€ëŠ” ê²ƒ**ì´ ê¸°ê³„(Machine)ê°€ í•™ìŠµ(Learning)í•˜ëŠ” ë°©ë²•ì´êµ¬ë‚˜!

### ì •ë¦¬

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì„ í˜• ëª¨ë¸(Linear model)ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì •ë¦¬í•´ ë³´ì.  
> 1. \\(\\hat{y}=wx+b\\) í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.  
> 2. ì†ì‹¤ í•¨ìˆ˜ (Loss function)ë¥¼ ì •ì˜í•œë‹¤. ì—¬ê¸°ì„œëŠ” **í‰ê·  ì œê³± ì˜¤ì°¨(mean squared error)** ë¥¼ ì‚¬ìš©í•œë‹¤.  
> 3. ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ëŠ” ë°©ë²•ì„ ì„ íƒí•œë‹¤. ì—¬ê¸°ì„œëŠ” **ê²½ì‚¬ í•˜ê°•ë²•(gradient descent)** ì„ ì‚¬ìš©í•œë‹¤. ê·¸ë¦¬ê³  ìµœì†Œê°’ì„ ì°¾ëŠ” ë°©ë²•(ì•Œê³ ë¦¬ì¦˜)ì„ **ì˜µí‹°ë§ˆì´ì €(Optimizer)** ë¼ê³  ë¶€ë¥´ê² ë‹¤.  
> 4. ë°˜ë³µí•  íšŒìˆ˜(epoch)ë¥¼ ê²°ì •í•œë‹¤.  
> 5. ì£¼ì–´ì§„ ì¡°ê±´ìœ¼ë¡œ ëª¨ë¸ì„ ìµœì í™”(fit) ì‹œí‚¨ë‹¤.  

## í…ì„œí”Œë¡œìš°(Tensorflow)ë¡œ ëª¨ë¸ë§(Modeling)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì¢‹ì•˜ì–´!   
> 
> ì´ì œ í…ì„œí”Œë¡œìš°(Tensorflow)ê°€ ëƒ ëƒ í•œ   
> **ì¼€ë¼ìŠ¤(Keras)**ë¥¼ ì´ìš©í•´ì„œ êµ¬í˜„ì„ í•´ë³´ì!

### ì •ê·œí™”(Normalization)

âš™ï¸ ì—”ì§€ë‹ˆì–´  

> ë‚´ê°€ í•´ë´ì„œ ì•„ëŠ”ë°...   
> 
> ì´ê±° ì•ˆí•˜ë©´ ì—‰ëš±í•œ ë‹µì´ ë‚˜ì˜¨ë‹¤.
> ê¼­ í•´ì•¼ë¨!
>  
> **ì •ê·œê°’ = (í˜„ì¬ê°’ - ìµœì†Œê°’) / (ìµœëŒ€ê°’-ìµœì†Œê°’)** ìœ¼ë¡œ ì •ê·œí™” í•œë‹¤!  
> 
> ê·¸ë˜í”„ë¥¼ ë³´ë©´,  
> ë°ì´í„°ì˜ ëª¨ì–‘ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œë„  
> xì¶•ì˜ ê°’ì´ 0ì—ì„œ 1ì‚¬ì´ë¡œ ë³€í™˜ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.  


```python
from sklearn import preprocessing

mm_scaler = preprocessing.MinMaxScaler()
X_train = mm_scaler.fit_transform(x)
Y_train = mm_scaler.transform(y)

plt.scatter(X_train, Y_train)
plt.xlabel('scaled-height')
plt.ylabel('scaled-weight')
plt.show()
```


![png](output_28_0.png)


### Kerasë¥¼ ê°€ì§€ê³  ëª¨ë¸ë§(Modeling)í•˜ê¸°

âš™ï¸ ì—”ì§€ë‹ˆì–´

> 4ì¤„ë¡œ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ë‹¤!   
> 
> ì¼€ë¼ìŠ¤ ë§Œì„¸!


```python
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# ëª¨ë¸ì„ ì¤€ë¹„í•œë‹¤.
model = Sequential()

# ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ 1ì´ê³  ì¶œë ¥ ê°œìˆ˜ê°€ 1ì¸ y=wx+b ë¥¼ ìƒì„±í•œë‹¤.
model.add(Dense(1, input_dim=1))

# Loss funtionê³¼ Optimizerë¥¼ ì„ íƒí•œë‹¤.
model.compile(loss='mean_squared_error', optimizer='sgd') 

# epochsë§Œí¼ ë°˜ë³µí•´ì„œ ì†ì‹¤ê°’ì´ ìµœì €ê°€ ë˜ë„ë¡ ëª¨ë¸ì„ í›ˆë ¨í•œë‹¤.
hist = model.fit(X_train, Y_train, epochs=3000, verbose=0) 
```

    WARNING: Logging before flag parsing goes to stderr.
    W0827 13:24:13.240252 4570310080 deprecation.py:506] From /Users/skettee/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor


### ì†ì‹¤ê°’ì˜ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ í™•ì¸


```python
plt.plot(hist.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```


![png](output_32_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ë¡ ì†ì‹¤(Loss)ì´ 0ì— ê°€ê¹ê²Œ ëœë‹¤.  
> êµ¿ì¡!

### \\(w\\)ì™€ \\(b\\)ê°’ì„ í™•ì¸


```python
w, b = model.get_weights()
w =  w[0][0]
b = b[0]
print('w: ', w)
print('b: ', b)
```

    w:  0.8987676
    b:  -2.6254048


### ê·¸ë˜í”„ë¡œ í™•ì¸


```python
x_scale = mm_scaler.transform(x)
y_scale = mm_scaler.transform(y)
plt.scatter(x_scale, y_scale)
plt.plot(x_scale, w*x_scale+b, 'r')
plt.xlabel('scaled-height')
plt.ylabel('scaled-weight')
plt.show()
```


![png](output_37_0.png)


## í•´ê²° (Solution)

âš™ï¸ ì—”ì§€ë‹ˆì–´  

> ê³ ê°ë‹˜~ ì›í•˜ì‹œëŠ” ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.   
> input_heightì— ì›í•˜ì‹œëŠ” í‚¤ë¥¼ ì…ë ¥í•˜ì‹œë©´    
> 'ëª¸ì§±ë°˜'ì— ë“¤ì–´ ê°ˆ ìˆ˜ ìˆëŠ” ëª¸ë¬´ê²Œê°€ ìë™ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.    


```python
input_height = 170.0

input_x = mm_scaler.transform(np.array([input_height]).reshape(-1, 1))
predict = model.predict(input_x)
predict = mm_scaler.inverse_transform(predict)

print('ëª¸ì§±ë°˜ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ëª¸ë¬´ê²ŒëŠ” {:.2f} kg ì…ë‹ˆë‹¤'.format(predict[0][0]))
```

    ëª¸ì§±ë°˜ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ëª¸ë¬´ê²ŒëŠ” 62.96 kg ì…ë‹ˆë‹¤

