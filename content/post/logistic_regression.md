
+++
title = "ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression)"
date = "2019-8-16"
author = "skettee"
categories = ["Deep Learning", "Logistic Regression"]
tags = ["ë”¥ëŸ¬ë‹", "ë¡œì§€ìŠ¤í‹±íšŒê·€", "ì‹œê·¸ëª¨ì´ë“œ", "í™œì„± í•¨ìˆ˜", "Cross entropy loss", "RMSProp"]
+++



ë”¥ëŸ¬ë‹ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°€ê¸° ìœ„í•´ ì•Œì•„ì•¼ í•˜ëŠ” ë‘ë²ˆì§¸ ëª¨ë¸ì¸ ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)ì— ëŒ€í•´ ì•Œì•„ë³´ê³  kerasë¥¼ ì´ìš©í•´ì„œ ëª¨ë¸ë§ì„ í•´ë³´ì!
<!--more-->

ì‹¤ì œë¡œ ëŒë ¤ ë³´ê³  ì‹¶ìœ¼ë©´ êµ¬ê¸€ ì½”ë©ìœ¼ë¡œ ~  

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/skettee/notebooks/blob/master/logistic_regression.ipynb)



## ë¬¸ì œ (Problem)

ğŸ’° ê³ ê°

> ë•ë¶„ì— 'ëª¸ì§±ë°˜'ì— ë“¤ì–´ ê°”ì–´ìš”. ê³ ë§ˆì›Œìš”!  
>
> í•™êµì— ì•„ì£¼ ì¸ê¸°ê°€ ë§ì€ ì—¬í•™ìƒì´ ìˆì–´ìš”~   
> ê·¸ëŸ°ë° ì´ ì—¬í•™ìƒê³¼ ì¹´í†¡ ì¹œêµ¬ ë§ºê¸°ê°€ ì–´ë ¤ì›Œìš”.  
> ì•„ë§ˆ í‚¤í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¹œì¶”ë¥¼ í•˜ëŠ”ê±° ê°™ì•„ìš”.  
> ì´ ì—¬í•™ìƒì´ ì¹œì¶”í•œ ì‚¬ëŒì˜ í‚¤ì™€ ê±°ì ˆí•œ ì‚¬ëŒì˜ í‚¤ ë°ì´í„°ë¥¼ ê°€ì§€ê³   
> ê·¸ ì—¬í•™ìƒì´ ë‚˜ë¥¼ ì¹œì¶”í• ì§€ ê±°ì ˆí• ì§€ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.  
>  
> ë°ì´í„°ëŠ” ì•„ë˜ì— ìˆì–´ìš”.


```python
height_data = [150.0, 150.8, 151.6, 152.4, 153.2, 154.0, 154.8, 155.7, 156.5, 157.3, 158.1, 158.9, 159.7, 160.6, 161.4, 162.2, 163.0, 163.8, 164.6, 165.5, 166.3, 167.1, 167.9, 168.7, 169.5, 170.4, 171.2, 172.0, 172.8, 173.6, 174.4, 175.3, 176.1, 176.9, 177.7, 178.5, 179.3, 180.2, 181.0, 181.8, 182.6, 183.4, 184.2, 185.1, 185.9, 186.7, 187.5, 188.3, 189.1, 190.0]
chinchu_data = ['no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']
```

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë°ì´í„° ë¶„ì„ì´ í¬ê²Œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒ ê°™ì§€ê°€ ì•Šê³ ...  
> ì¼ë‹¨ ì¹œì¶”ë¥¼ ìš”ì²­í•˜ë©´ ê²°ê³¼ë¥¼ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆì„ê²ƒ ê°™ì€ë°...

ğŸ’°ğŸ’° ê³ ê°

> ë”ë¸”!

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì§€ê¸ˆ ë°”ë¡œ ë¶„ì„ ë“¤ì–´ê°‘ë‹ˆë‹¤~

## ë°ì´í„° ë¶„ì„ (Data Analysis)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë°ì´í„°ê°€ ì–´ë–¤ ëª¨ì–‘ì¸ì§€    
> í™•ì¸í•´ ë³´ì•„ì•¼ ê² êµ°



```python
%matplotlib inline

import matplotlib.pyplot as plt

plt.scatter(height_data, chinchu_data)
plt.xlabel('height (cm)')
plt.ylabel('chinchu (yes or no)')
plt.show()
```


![png](output_6_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì•„...  
> ì•ì—ì„œ ë°°ìš´ 'ì„ í˜• íšŒê·€'ë¡œëŠ”  
> ë‹µì´ ë‚˜ì˜¬ ê²ƒ ê°™ì§€ ì•Šë‹¤...  

## ë°ì´í„° ë³€í™˜ (Data Transformation)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> í‚¤ì™€ ì¹œì¶” ë°ì´í„°ë¥¼ ê°ê° ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•˜ì  
> í–‰ì˜ í¬ê¸°ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜    
> ì—´ì˜ í¬ê¸°ëŠ” ì¸¡ì •í•œ í•­ëª©ì˜ ê°œìˆ˜    
>  
> í‚¤ ê°’ì„ ì…ë ¥í•˜ë©´ 'ì¹œì¶”'ê°€ëŠ¥ ì—¬ë¶€(yes or no)ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•˜ë‹ˆê¹Œ...  
> í‚¤ ë°ì´í„°ë¥¼ ì…ë ¥ xë¼ê³  í•˜ê³  ì¹œì¶” ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ì¶œë ¥ yë¼ê³  í•˜ì  
>
> í‚¤ ë°ì´í„°ëŠ” 50ê°œì˜ 'í‚¤'ë¥¼ ì¸¡ì •í•œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ 50X1 ë§¤íŠ¸ë¦­ìŠ¤   
> ì¹œì¶” ë°ì´í„°ëŠ” 50ê°œì˜ 'ì¹œì¶” ì—¬ë¶€'ë¥¼ ì¸¡ì •í•œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ 50X1 ë§¤íŠ¸ë¦­ìŠ¤ì´ë‹¤.  
>
> ì†ì‹¤ í•¨ìˆ˜(Loss function)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ”   
> 'yes', 'no'ë¥¼ ê³„ì‚°ì´ ê°€ëŠ¥í•œ ìˆ˜ë¡œ í‘œì‹œí•´ì•¼ í•œë‹¤.   
> ì—¬ê¸°ì—ì„œëŠ” 'yes'ì™€ 'no' ë‘ê°€ì§€ì˜ ê²°ê³¼ë§Œ ìˆìœ¼ë‹ˆê¹Œ      
> 'yes'ë¥¼ 1ë¡œ, 'no'ë¥¼ 0ìœ¼ë¡œ ë³€í™˜í•œë‹¤.  


```python
def transform_y(y):
    if y == 'yes':
        return 1
    else:
        return 0

import numpy as np

x = np.array(height_data).reshape(len(height_data), 1)
y = np.array([transform_y(i) for i in chinchu_data ]).reshape(len(chinchu_data), 1)

plt.scatter(x, y)
plt.xlabel('height (cm)')
plt.ylabel('chinchu')
plt.show()
```


![png](output_9_0.png)


## ëª¨ë¸ë§ (Modeling)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë”±ë´ë„ ì„ í˜• íšŒê·€(Linear Regression)ëª¨ë¸ë§ì€ ë‹µì´ ì•„ë‹ˆë‹¤...   
> \\(y=wx+b\\) ë° \\(J(w,b)\\), ê·¸ë¦¬ê³  ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì„ ì´ìš©í•˜ë©´ì„œ   
> ìœ„ì™€ ë¹„ìŠ·í•œ ê·¸ë˜í”„ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë¸ë§ì´  
> ê³¼ì—° ì¡´ì¬í• ê¹Œ?

**ê³„ë‹¨ í•¨ìˆ˜ (step function)**

ìœ„ì˜ ë°ì´í„° ë¶„í¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ëª¨ì–‘ì„ ê°€ì§€ëŠ” ê³„ë‹¨ í•¨ìˆ˜ë¥¼ ìƒê°í•´ ë³´ì.  

\\(y=s\_{c}(x)\\\\ 
\\\\ 
s\_{c}(x) = 
\\begin{cases}
0, & x < c \\\\ 
1, & x \\ge c
\\end{cases}\\)


```python
plt.step(x, y)
plt.show()
```


![png](output_12_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì˜¤! ë­”ê°€ ê·¸ëŸ´ì‹¸í•œë°...  
> \\(z=wx+b\\)ë¡œ ë†“ê³   
> \\(\\hat{y}=s\_{c}(z)\\)ë¡œ ëª¨ë¸ë§í•´ì„œ  
> \\(J(w,b)\\) í•¨ìˆ˜ë¥¼ ê·¸ë ¤ë³´ì  


```python
import numpy as np
from sklearn.metrics import mean_squared_error
from mpl_toolkits import mplot3d

# w,dì˜ ë²”ìœ„ë¥¼ ê²°ì •í•œë‹¤.
w = np.arange(-10, 10, 0.1)
d = np.arange(160, 180, 1)
j_array = []

# (20, 200) ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.
W, D = np.meshgrid(w, d)

# w, bë¥¼ í•˜ë‚˜ì”© ëŒ€ì‘í•œë‹¤.
for we, de in zip(np.ravel(W), np.ravel(D)):
    z_hat = np.multiply(we, x)
    y_list = []
    for ze in z_hat:
        if ze < de:
            y_list.append(0)
        else:
            y_list.append(1)
    y_hat = np.array(y_list)
    # Cost function
    mse = mean_squared_error(y_hat, y) / 2.0
    j_array.append(mse)

# ì†ì‹¤(Loss)ì„ êµ¬í•˜ê³  (20, 200) ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.
J = np.array(j_array).reshape(W.shape)

# ì„œí”¼ìŠ¤ ê·¸ë˜í”„ë¥¼ ê·¸ë¦°ë‹¤.
fig = plt.figure()
ax = plt.axes(projection="3d")

ax.plot_surface(W, D, J, color='b', alpha=0.5)
ax.set_xlabel('w')
ax.set_ylabel('d')
ax.set_zlabel('J')
plt.show()
```


![png](output_14_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> ê·¸ë§Œ ì•Œì•„ë³´ì...  
> ê²°ë¡ ì ìœ¼ë¡œ ê³„ë‹¨ í•¨ìˆ˜(Step function)ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤!  
> ì™œëƒí•˜ë©´...  
>  
> \\(J(w,b)\\)ê°€ **ë¯¸ë¶„ ë¶ˆê°€ëŠ¥** í•˜ê¸° ë•Œë¬¸ì´ë‹¤!  
>  
> ë¯¸ë¶„ ê°€ëŠ¥í•˜ë©´ì„œë„ ê³„ë‹¨ í•¨ìˆ˜ì™€ ë¹„ìŠ·í•œ í•¨ìˆ˜ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤...  
>  
> ë‹¤í–‰íˆ  
> **Së¼ì¸**ì˜ ë©‹ì§„ í•¨ìˆ˜ê°€ ìˆë‹¤!  
>  
> ê·¸ê²ƒì€ ë°”ë¡œ~

### ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (Sigmoid function)

\\(\\sigma(x) = {1 \\over {1 + e^{-x}}}\\)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì¿ í¼ í˜•ë‹˜ì€ ì´ëŸ° ë§ì„ í•˜ì…¨ì§€   
> **'ìš°ë¦° ë‹µì„ ì°¾ì„ ê±°ì•¼, ëŠ˜ ê·¸ë¬ë“¯ì´'**  
> 
> ì—”ì§€ë‹ˆì–´ë“¤ì€  
> ë¯¸ë¶„ì´ ì˜ ë˜ê³   
> xì— ì–´ë–¤ ê°’ì„ ë„£ì–´ë„ 0ì´ë‚˜ 1ì˜ ê°’ì— ê°€ê¹Œìš´ ê°’ì„ ê°€ì§€ëŠ”  
> í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ëƒˆë‹¤.  
> ì´ê²ƒì´ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë‹¤.  


```python
sigmoid = lambda x: 1.0 / (1.0 + np.exp(-x))

xx = np.linspace(-10,10,100)

plt.plot(xx, sigmoid(xx))
plt.show()
```


![png](output_17_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì˜¤! ë©‹ì§„ë°...  
> \\(z=wx+b\\)ë¡œ ë†“ê³   
> \\(\\hat{y}=\\sigma(z)\\)ë¡œ ëª¨ë¸ë§í•´ì„œ  
> \\(J(w,b)\\) í•¨ìˆ˜ë¥¼ ê·¸ë ¤ë³´ì   
>
> ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜(Loss function)ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤.  

### ë¶„ë¥˜(Classification)ë¥¼ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜: Cross-entropy Loss

í•˜ë‚˜ì˜ ë°ì´í„° ì„¸íŠ¸(\\(x^{(i)}, y^{(i)}\\))ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì—ì„œ ì–»ì€ ê°’ê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´(Loss)ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í•´ë³´ì.  
ì—¬ê¸°ì„œ \\(x^{(i)}\\)ëŠ” ië²ˆì§¸ \\(x\\)ê°’ì´ê³  \\(y^{(i)}\\)ì€ ië²ˆì§¸ \\(y\\)ê°’ì´ë‹¤.  

ì¼ë‹¨ \\(w\\)ì™€ \\(b\\)ëŠ” ì„ì˜ì˜ ê°’ìœ¼ë¡œ ë†“ì. ê·¸ë¦¬ê³  ëª¨ë¸ì— \\(x^{(i)}\\)ì„ ë„£ê³  ê³„ì‚°í•œ ê²°ê³¼ ê°’ \\({\\hat y}^{(i)}\\)ê³¼ ì‹¤ì œ ê°’ \\(y^{(i)}\\)ì˜ ì°¨ì´ë¥¼ êµ¬í•œë‹¤. ë¡œì§€ìŠ¤í‹± ëª¨ë¸ë§ì—ì„œëŠ” í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤(Cross-entropy loss)ì„ ì‚¬ìš©í•œë‹¤.

\\(L({\\hat y}^{(i)}, y^{(i)})=-\\bigl (y^{(i)}log({\\hat y}^{(i)}) + (1-y^{(i)})log(1-{\\hat y}^{(i)})\\bigr)\\)

ëª¨ë“  ë°ì´í„°(mê°œì˜ ë°ì´í„° ì„¸íŠ¸)ë¡œ ë¶€í„° ì–»ì€ ê²ƒì„ í‰ê·  í•œê²ƒì´ ì†ì‹¤ í•¨ìˆ˜(Loss function)ì´ë‹¤. ì†ì‹¤ í•¨ìˆ˜ëŠ” \\(w\\)ì™€ \\(b\\)ì˜ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

\\({\\large J}(w, b) = {1\\over m}\\sum\_{i=1}^m L({\\hat y}^{(i)}, y^{(i)}) \\\\ 
\\\\ 
\\hspace{2.9em}= -{1\\over {m}}\\sum\_{i=1}^m [(y^{(i)}log({\\hat y}^{(i)})) + (1-y^{(i)})log(1-{\\hat y}^{(i)})]\\)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì´ì œë¶€í„°  
> ì„ í˜• íšŒê·€(Linear regression) ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ ì†ì‹¤ í•¨ìˆ˜ì¸   
> í‰ê·  ì œê³± ì˜¤ì°¨ (mean squared error)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³    
> ê³ ì–‘ì´ëƒ ê°œëƒ í† ë¼ëƒ, Yes or No ë“±ì˜ ë¶„ë¥˜(Classification) ë¬¸ì œë¥¼ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ë¡œì„œ  
> ì•„ë˜ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.  
>
> \\({\\large J}(w, b) = -{1\\over {m}}\\sum\_{i=1}^m [(y^{(i)}log({\\hat y}^{(i)})) + (1-y^{(i)})log(1-{\\hat y}^{(i)})]\\)   
>
> ì—”ì§€ë‹ˆì–´ë¥¼ ê°ˆì•„ì„œ ë§Œë“  ê²ƒì´ë‹ˆ  
> ìš°ë¦¬ëŠ” ì‚¬ìš©í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤  

**ì†ì‹¤ í•¨ìˆ˜ (Loss function) ì‹œê°í™”**

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ìš°ì„  ì†ì‹¤ í•¨ìˆ˜(Loss function)ê°€ ì–´ë–»ê²Œ ìƒê²¨ ë¨¹ì—ˆëŠ”ì§€ ì‚´í´ ë³´ì.  
> xì¶•ì„ \\(w\\)ë¡œ ë†“ê³ , yì¶•ì„ \\(b\\)ë¡œ ë†“ê³ , zì¶•ì„ ì†ì‹¤ í•¨ìˆ˜ \\({\\large J}(w, b)\\)ë¡œ 
> ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ ë³´ë©´  
> ì–´ë–»ê²Œ ìµœì†Œê°’ì„ ì°¾ì„ì§€ ê°ì´ ì˜¬ ê²ƒ ê°™ë‹¤.   


```python
from sklearn.metrics import log_loss

cross_entropy_loss = True

# W,bì˜ ë²”ìœ„ë¥¼ ê²°ì •í•œë‹¤.
w = np.arange(20, 30, 0.1)
b = np.arange(-4595, -4585, 0.1)

j_loss = []

# ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.
W, B = np.meshgrid(w, b)

# w, bë¥¼ í•˜ë‚˜ì”© ëŒ€ì‘í•œë‹¤.
for we, be in zip(np.ravel(W), np.ravel(B)):
    z = np.add(np.multiply(we, x), be)
    y_hat = sigmoid(z)
    # Loss function
    if cross_entropy_loss: 
        loss = log_loss(y, y_hat) # Log loss, aka logistic loss or cross-entropy loss.
        j_loss.append(loss)
    else:
        loss = mean_squared_error(y_hat, y) / 2.0 # Mean squred error
        j_loss.append(loss)

# ì†ì‹¤(Loss)ì„ êµ¬í•œë‹¤.
J = np.array(j_loss).reshape(W.shape)

# ì„œí”¼ìŠ¤ ê·¸ë˜í”„ë¥¼ ê·¸ë¦°ë‹¤.
fig = plt.figure()
ax = plt.axes(projection="3d")
ax.plot_surface(W, B, J, color='b', alpha=0.5)
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_zlabel('J')
plt.show()

```


![png](output_21_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> V ëª¨ì–‘ìœ¼ë¡œ êµ¬ë¶€ëŸ¬ì§„ ëª¨ì–‘ì´ë‹¤!  
> \\(w\\)ê°€ 27ê·¼ì²˜ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Œê°’ì„ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.  
> 
> ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Œê°€ ë˜ëŠ” \\(w\\)ì™€ \\(b\\)ë¥¼ ë¹ ë¥´ê²Œ ì°¾ê¸° ìœ„í•´ì„œ   
> ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ  
> **RMSProp** ì„ ì‚¬ìš©í•œë‹¤.

### RMSProp

âš™ï¸ ì—”ì§€ë‹ˆì–´

> RMSPropì˜ ì›ë¦¬ëŠ”  
> ê¸‰ê²½ì‚¬ì¸ ê²½ìš°ì—ëŠ” ë³´í­ì„ ë‚®ì¶”ì–´ì„œ ê°€ì¥ ì•„ë˜ì¸ì§€ë¥¼ ì„¸ë°€íˆ ì‚´í”¼ê³     
> ì™„ë§Œí•œ ê²½ì‚¬ì¸ ê²½ìš°ì—ëŠ” ë³´í­ì„ ë„“í˜€ì„œ ë¹¨ë¦¬ ì§€ë‚˜ê°€ëŠ” ë°©ì‹ì´ë‹¤.  
> ì´ ë°©ì‹ì€ ë§¤ìš° ë¹ ë¥´ê²Œ ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.  
>
> ì´ê²ƒë„ ì—”ì§€ë‹ˆì–´ë¥¼ ê°ˆì•„ì„œ ë§Œë“  ê²ƒì´ë‹ˆ  
> ìš°ë¦¬ëŠ” ì‚¬ìš©í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤  
>
> \\(dw = {\\partial {J(w,b)}\\over \\partial w}\\),  
> \\(db = {\\partial{J(w,b)}\\over \\partial b}\\)  
>
> REPEAT(epoch) {    
> \\(w:=w-\\alpha {dw \\over {\\sqrt {s\_{dw} + \\epsilon}}}\\)      
>
> \\(b:=b-\\alpha {db \\over {\\sqrt {s\_{db} + \\epsilon}}}\\)   
> }  
> 
> \\(S\_{dw} = \\rho S\_{dw} + (1-\\rho)dw^2\\),  
> \\(S\_{db} = \\rho S\_{db} + (1-\\rho)db^2\\)  
>
> \\(\\alpha=0.001\\) : learining rate,  
> \\(\\rho=0.9\\) : discounting factor,  
> \\(\\epsilon=1e-07\\) : small value to avoid zero denominator

### ì •ë¦¬

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë¡œì§€ìŠ¤í‹± ëª¨ë¸(Logistic model)ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì •ë¦¬í•´ ë³´ì.  
> 1. \\(z=wx+b\\) í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.  
> 2. \\(a = \\sigma(z)\\) í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤. \\(a\\)ë¥¼ **í™œì„± í•¨ìˆ˜(activation function)** ë¼ê³  í•œë‹¤.  
> 3. \\(\\hat{y} = a\\) ì´ë‹¤.
> 2. ì†ì‹¤ í•¨ìˆ˜ (Loss function)ë¥¼ ì •ì˜í•œë‹¤. ì—¬ê¸°ì„œëŠ” **í¬ë¡œìŠ¤-ì—”íŠ¸ë¡œí”¼ ì†ì‹¤(cross-entropy loss)** ë¥¼ ì‚¬ìš©í•œë‹¤.  
> 3. ì˜µí‹°ë§ˆì´ì €(Optimizer)ë¥¼ ì„ íƒí•œë‹¤. ì—¬ê¸°ì„œëŠ” **RMSProp**ì„ ì‚¬ìš©í•œë‹¤.  
> 4. ë°˜ë³µí•  íšŒìˆ˜(epoch)ë¥¼ ê²°ì •í•œë‹¤.  
> 5. ì£¼ì–´ì§„ ì¡°ê±´ìœ¼ë¡œ ëª¨ë¸ì„ ìµœì í™”(fit) ì‹œí‚¨ë‹¤.  

## í…ì„œí”Œë¡œìš°(Tensorflow)ë¡œ ëª¨ë¸ë§(Modeling)

âš™ï¸ ì—”ì§€ë‹ˆì–´

> ì¢‹ì•˜ì–´!   
> 
> ì´ì œ êµ¬ê¸€(Google)ì´ ëƒ ëƒ í•œ   
> **ì¼€ë¼ìŠ¤(Keras)**ë¥¼ ì´ìš©í•´ì„œ êµ¬í˜„ì„ í•´ë³´ì!

### ì •ê·œí™” (Normalization)

âš™ï¸ ì—”ì§€ë‹ˆì–´  

> ì´ì œ ì •ê·œí™”ëŠ”  
> ì„ íƒì´ ì•„ë‹Œ **í•„ìˆ˜**!  
> 
> **ì •ê·œê°’ = (í˜„ì¬ê°’ - ìµœì†Œê°’) / (ìµœëŒ€ê°’-ìµœì†Œê°’)** ìœ¼ë¡œ ì •ê·œí™” í•œë‹¤!  
> 
> ê·¸ë˜í”„ë¥¼ ë³´ë©´,  
> ë°ì´í„°ì˜ ëª¨ì–‘ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œë„  
> xì¶•ì˜ ê°’ì´ 0ì—ì„œ 1ì‚¬ì´ë¡œ ë³€í™˜ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.  


```python
from sklearn import preprocessing

mm_scaler = preprocessing.MinMaxScaler()
X_train = mm_scaler.fit_transform(x)
Y_train = y

plt.scatter(X_train, Y_train)
plt.xlabel('scaled-height')
plt.ylabel('chinchu')
plt.show()
```


![png](output_27_0.png)


### Kerasë¥¼ ê°€ì§€ê³  ëª¨ë¸ë§(Modeling)í•˜ê¸°

âš™ï¸ ì—”ì§€ë‹ˆì–´

> 4ì¤„ë¡œ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ë‹¤!   
> 
> ì¼€ë¼ìŠ¤ ë§Œë§Œì„¸!


```python
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation

# ëª¨ë¸ì„ ì¤€ë¹„í•œë‹¤.
model = Sequential()

# ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ 1ì´ê³  ì¶œë ¥ ê°œìˆ˜ê°€ 1ì¸ y=sigmoid(wx+b)ë¥¼ ìƒì„±í•œë‹¤.
model.add(Dense(1, input_dim=1, activation='sigmoid'))

# Loss funtionê³¼ Optimizerë¥¼ ì„ íƒí•œë‹¤.
model.compile(loss='binary_crossentropy', optimizer='rmsprop') 

# epochsë§Œí¼ ë°˜ë³µí•´ì„œ ì†ì‹¤ê°’ì´ ìµœì €ê°€ ë˜ë„ë¡ ëª¨ë¸ì„ í›ˆë ¨í•œë‹¤.
hist = model.fit(X_train, Y_train, epochs=10000, batch_size=20, verbose=0) 
```

### ì†ì‹¤ê°’ì˜ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ í™•ì¸


```python
plt.plot(hist.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
```


![png](output_31_0.png)


âš™ï¸ ì—”ì§€ë‹ˆì–´

> ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ë¡ ì†ì‹¤(Loss)ì´ 0ì— ê°€ê¹ê²Œ ëœë‹¤.  
> ë‚˜ì´ìŠ¤!

### \\(w\\)ì™€ \\(b\\)ê°’ì„ í™•ì¸


```python
w, b = model.get_weights()
w =  w[0][0]
b = b[0]
print('w: ', w)
print('b: ', b)
```

    w:  14.634907
    b:  -7.5143723


### ê·¸ë˜í”„ë¡œ í™•ì¸


```python
x_scale = mm_scaler.transform(x)
plt.scatter(x_scale, y)
plt.plot(x_scale, sigmoid(w*np.array(x_scale)+b), 'r')
plt.xlabel('scaled-height')
plt.ylabel('chinchu')
plt.show()
```


![png](output_36_0.png)


## í•´ê²° (Solution)

âš™ï¸ ì—”ì§€ë‹ˆì–´  

> ê³ ê°ë‹˜~ ì›í•˜ì‹œëŠ” ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.   
> input_heightì— ì›í•˜ì‹œëŠ” í‚¤ë¥¼ ì…ë ¥í•˜ì‹œë©´    
> 'ì¹œì¶”'ê°€ ë  í™•ë¥ ì„ ì•Œë ¤ ì¤ë‹ˆë‹¤.


```python
input_height = 178.0

input_x = mm_scaler.transform(np.array([input_height]).reshape(-1, 1))
predict = model.predict(input_x)

print('ì¹œì¶”ê°€ ë  í™•ë¥ ì€ {:.1f}% ì…ë‹ˆë‹¤.'.format(predict[0][0]*100))
```

    ì¹œì¶”ê°€ ë  í™•ë¥ ì€ 93.9% ì…ë‹ˆë‹¤.

